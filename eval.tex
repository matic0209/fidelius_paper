\section{Evaluation}\label{sec:eval}
\input{figures/algo_attestation}

In this section, we assess Fidelius's performance through comprehensive experiments. Initially, we gauge its time consumption by executing CPU-intensive and I/O-intensive tasks separately. Subsequently, we conduct a performance comparison between Fidelius and alternative solutions that execute analysis programs in the Ethereum Virtual Machine (EVM). Our experiments are conducted on a machine equipped with an Intel(R) Core(TM) i5-10400F CPU boasting 12 cores at 2.9 GHz, 16 GBytes of RAM, and a 12 MByte cache. Unless otherwise stated, each experiment is repeated 1,000 times to calculate the average value.

\subsection{CPU-Intensive Task}
To evaluate Fidelius's performance on CPU-intensive tasks, we implement a neural network algorithm with three layers and 128 hidden units, designed to recognize handwritten digits using the MNIST dataset~\cite{mnist}. We report both the time consumption and accuracy of the algorithm. Additionally, we deploy the algorithm on raw CPU (without SGX) for comparison. The key differences between the Fidelius-based algorithm and the raw CPU-based algorithm include:
\begin{itemize}
  \item Random library. The Fidelius-based algorithm utilizes a random library based on Intel SGX, whereas the raw CPU-based algorithm relies on the C++ random library.
  \item Data decryption. Fidelius requires decryption of sealed data before initiating the algorithm, incurring additional processing time. Conversely, the raw CPU-based algorithm can directly access plaintext data.
\end{itemize}
Both for Fidelius and raw CPU, we maintain a fixed number of test sets at 1,000 while incrementally increasing the number of training sets from 10,000 to 50,000. For each training set, we execute the algorithm with epochs set to 100 and 200, respectively.

\input{figures/mnist.tex}

Figures~\ref{fig:cpu_intensive}(a) and~\ref{fig:cpu_intensive}(b) illustrate the time consumption of the algorithms running on Fidelius and raw CPU with varying epochs. As the number of training sets increases, the time consumption of the algorithms on both Fidelius and raw CPU shows a linear growth trend. However, the time consumption difference between these two algorithms is minimal. Specifically, it is observed that the execution time on Fidelius is only 2\% longer than that of raw CPU, primarily due to data decryption overhead.

Figures~\ref{fig:cpu_intensive}(c) and~\ref{fig:cpu_intensive}(d) show the accuracy of algorithms on Fidelius and raw CPU over different epochs, with both achieving over 91\% accuracy. At 100 epochs, accuracy fluctuates slightly, but with no marked difference between the platforms. At 200 epochs, both algorithms converge with nearly identical accuracies. Fidelius shows a marginal 2\% increase in time consumption compared to raw CPU but maintains similar accuracy levels, demonstrating comparable performance in CPU-intensive tasks.


\subsection{I/O-Intensive Task}\label{subsec:io_intensive}
To evaluate the time efficiency of Fidelius in handling I/O-intensive tasks, we tested an algorithm that searches for a target string within a file over 1 GByte on both Fidelius and raw CPU platforms. Time consumption depends on data loading and string searching, with additional time required on Fidelius due to data decryption. Given Intel SGX's constraints in Fidelius, we set data loading block sizes to 64 KBytes and 256 KBytes and varied the number of data lines from 1 to 128 and 1 to 256 for each block size to measure time efficiency.


\input{figures/personlist.tex}
Figure~\ref{fig:io_intensive} shows the time efficiency of Fidelius compared to raw CPU across different data block sizes. Fidelius shows a significant reduction in time consumption as the number of lines read increases. For instance, with a block size of 64 KBytes, the time taken by Fidelius drops from 81 seconds (for 1 line) to 11 seconds (for 128 lines). Similarly, with a block size of 256 KBytes, it reduces from 213 seconds (for 1 line) to 10 seconds (for 256 lines). Increasing the block size to 256 KBytes further optimizes Fidelius's performance. Despite improvements, a gap remains in time consumption between Fidelius and raw CPU, largely due to the decryption process in I/O-intensive tasks.

\subsection{Performance Comparison}\label{subsec:evm_cmp}
In this section, we compare the performance of Fidelius with SDTE~\cite{dai2019sdte}, which uses a \textit{k}-nearest neighbors (\textit{k}-NN) algorithm as an Ethereum smart contract to execute machine learning tasks on the Ethereum Virtual Machine (EVM). We implement the \textit{k}-NN algorithm on both Fidelius and raw CPU to evaluate each solution's time consumption, using the Titanic~\cite{titanic} dataset from Kaggle as input.

To evaluate the \textit{k}-NN algorithm's execution time on the Ethereum Virtual Machine (EVM), we deployed the \textit{k}-NN smart contract on Ganache, a personal Ethereum blockchain. The reported time consumption for \textit{k}-NN on the EVM focuses solely on the execution time, excluding any time spent on transaction broadcasting or committing. We also exclude the time required to upload the Titanic dataset to the blockchain, as the \textit{k}-NN contract uses on-chain data. Due to EVM's memory limits, the \textit{k}-NN algorithm cannot handle more than 60 test sets, hence we limit test sets to 10-60 while keeping training sets at 100.

\input{figures/knn.tex}
Figure~\ref{fig:knn_cmp} shows the time consumption of the \textit{k}-NN algorithm on raw CPU, Fidelius, and the Ethereum Virtual Machine (EVM), with the latter taking roughly 30 times longer than the others. Additionally, EVM executions often fail when testing more than 60 sets due to memory constraints, highlighting the difficulties of running complex machine learning algorithms with large datasets on the EVM, as also noted in previous studies about Ethereum's memory limits~\cite{dinh2017blockbench}. In contrast, Fidelius provides a more reliable and stable environment for sophisticated machine learning tasks.
